{"cells":[{"cell_type":"markdown","metadata":{"id":"MPjY0C8lSSTf"},"source":["### **Mount your Google Drive.**\n","Make sure your workspace containing processed data is added to the drive. <br>\n","\n","**Navigate to Runtime and change the settings to T4 GPU**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3AD5h_9iCJYi"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"bcBf2ezzvJj5"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"EYXzDBzWStUs"},"source":["#### **You can run the below cells to verify whether the folder exists on the drive**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"94JP3Yp1MM3l"},"outputs":[],"source":["%cd /content/drive/MyDrive/hw10_workspace/src/model_generation\n","!ls"]},{"cell_type":"markdown","metadata":{"id":"CytytazOvJj6"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"t0hxCcm9UAOR"},"source":["### **Install dependencies, and import necessary libraries**\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d-m3m0W5VFkg"},"outputs":[],"source":["import os, math, argparse, torch, random\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","from torch.cuda.amp import GradScaler, autocast\n","from torch.optim import AdamW\n","from torch.utils.tensorboard import SummaryWriter\n","from torchvision import transforms as T\n","from PIL import Image\n","\n","from dataset import make_loaders         # from dataset.py\n","from model import ImageOnlySteerNet      # from model.py\n","\n","IMAGENET_MEAN = (0.485, 0.456, 0.406)\n","IMAGENET_STD  = (0.229, 0.224, 0.225)"]},{"cell_type":"markdown","metadata":{"id":"VyVTVS-dvJj7"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"IKo8Uc9aUeme"},"source":["### **Data Augmentation And Preprocessing Pipeline**\n","In this section, you define how the input images are transformed\n","before being fed into the neural network.\n","There are two parts:<br>\n","- `make_eval_tf()` - transformations for validation/test (NO augmentation)\n","- `make_train_tf()` - transformations for training (WITH augmentation)\n","\n","The idea is to help the model generalize better to unseen lighting\n","conditions, camera shifts, or image distortions by applying random\n","transformations during training.\n","Run the below cell, you donot have to make any changes here."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ukRKBh6PYDKT"},"outputs":[],"source":["# ------------------ Augmentations ------------------\n","class TopCrop(torch.nn.Module):\n","    def __init__(self, frac: float): super().__init__(); self.frac = max(0.0, min(1.0, float(frac)))\n","    def forward(self, im: Image.Image):\n","        if self.frac <= 0: return im\n","        w, h = im.size; cut = int(h * self.frac)\n","        return im.crop((0, cut, w, h))\n","\n","def make_eval_tf(short_side: int, top_crop: float):\n","    return T.Compose([\n","        TopCrop(top_crop),\n","        T.Resize(short_side),          # keeps aspect ratio (short side -> short_side)\n","        T.CenterCrop(short_side),      # make square for ResNet\n","        T.ToTensor(),\n","        T.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n","    ])\n","\n","def make_train_tf(args):\n","    return T.Compose([\n","        TopCrop(args.top_crop),\n","        T.Resize(args.short_side),\n","        T.CenterCrop(args.short_side),\n","        T.ColorJitter(brightness=args.jitter_b,\n","                      contrast=args.jitter_c,\n","                      saturation=args.jitter_s,\n","                      hue=args.jitter_h),\n","        T.RandomAffine(\n","            degrees=args.affine_deg,\n","            translate=(args.affine_trans, args.affine_trans),\n","            scale=(args.affine_scale_min, args.affine_scale_max),\n","            interpolation=T.InterpolationMode.BILINEAR,\n","            fill=0,\n","        ),\n","        T.ToTensor(),\n","        T.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n","    ])\n","# ---------------------------------------------------"]},{"cell_type":"markdown","metadata":{"id":"n3jKfG2dvJj8"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"MP6-B3Jlb1DF"},"source":["### **Training Pipeline Set-Up**\n","#### MODEL EVALUATION FUNCTION\n","This helper function evaluates the trained model on a dataset.\n","It computes two metrics:\n","- Mean Absolute Error (MAE)\n","- Root Mean Squared Error (RMSE)\n","You do NOT need to modify this function.\n","\n","#### Helper: get_lr()\n","Returns the current learning rate from the optimizer.\n","Used for TensorBoard logging.\n","You do NOT need to modify this function."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-eLIbdMgkdce"},"outputs":[],"source":["@torch.no_grad()\n","def evaluate(model, loader, device, mu, sigma):\n","    model.eval()\n","    se = mae = n = 0\n","    for x, _, y_std, y_raw, _ in loader:\n","        x = x.to(device, non_blocking=True)\n","        y_std = y_std.to(device, non_blocking=True)\n","        yhat_std = model(x)                 # [B]\n","        yhat_raw = yhat_std * sigma + mu    # [B]\n","        diff = (yhat_raw.cpu() - y_raw)     # [B]\n","        mae += diff.abs().sum().item()\n","        se  += (diff**2).sum().item()\n","        n   += y_raw.shape[0]\n","    mae /= max(1, n)\n","    rmse = math.sqrt(se / max(1, n))\n","    return mae, rmse\n","\n","def get_lr(optimizer):\n","    for pg in optimizer.param_groups:\n","        return pg.get(\"lr\", None)"]},{"cell_type":"markdown","metadata":{"id":"9ZDIZPMfvJj9"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"lN34lO7kvJj9"},"source":["### **EXPERIMENT SETUP**\n","\n","The main training script below is where we: <br>\n","- Define hyperparameters\n","- Prepare dataloaders\n","- Initialize model, optimizer, and loss function\n","- Configure TensorBoard logging\n","\n","For the first iteration, you can keep the default values of the hyperparameters and train the model.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XHf6TATzvJj9"},"outputs":[],"source":["# ------------------ TODO: Define Hyperparameters ------------------\n","epochs = 1             # Number of epochs to train the model, try values ranging from 10-40 epochs\n","lr = 1                  # Learning Rate - value range to try -> (1e-3 to 1e-5)\n","wd = 1                  # Weight Decay - try values from -> (1e-3 to 1e-5)\n","dropout = 0.1              # Percentage of dropout\n","# -----------------------------------------------------------------\n","bs = 64                    # Batch Size for training\n","use_aug = True             # Flag for use of augmentation\n","seed = 42                  # Random seed\n","fp16 = True\n","pretrained = True\n","freeze_backbone = False\n","\n","logdir = \"runs/image_only\"\n","ckpt_out = \"ckpt_best.pt\"\n","\n","random.seed(seed)\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed_all(seed)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Device: {device}\")\n","args = argparse.Namespace(\n","    epochs=epochs,\n","    lr=lr,\n","    batch_size=bs,\n","    wd = wd,\n","    dropout=dropout,\n","    fp16=fp16\n",")"]},{"cell_type":"markdown","metadata":{"id":"Io5Ij2nzvJj9"},"source":["\n","The datset setup function is loaded from the dataset.py file. Add the paths of the merged_dataset **index_smooth.json** file and the path of the **root directory of merged_dataset** in the code snippet below."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MU21N3ZsvJj9"},"outputs":[],"source":["# ============================================================\n","# Dataset Setup\n","# ============================================================\n","omega_sign = +1.0\n","train_dl, val_dl, test_dl, stats = make_loaders(\n","    index_json=\"path/to/your/index_smooth.json\",\n","    root=\"path/to/your/merged_dataset\",\n","    bs=bs,\n","    hist_len=0,\n","    omega_sign=omega_sign,\n","    short_side=224,\n","    top_crop_frac=0.2\n","    )\n","mu, sigma = stats[\"mu\"], stats[\"sigma\"]\n","print(f\"Label standardization: mu={mu:.6f}, sigma={sigma:.6f}\")"]},{"cell_type":"markdown","metadata":{"id":"7CaMzq16vJj9"},"source":["Let us now apply data augmentation to increase the variability in the training data.\n","\n","*   top crop: Crops the top part of the frame as most of the tracks are on the bottom of the image. Try experimenting with the values within the provided range. You have define the percentage of the frame you want to crop\n","*   You can also try out different degrees of affine transformations\n","*   Among the available data augmentation techniques (rotation, translation, scaling, color jitter, and flipping), which transformations are expected to be most effective for improving model generalization in a vision-based self-driving context, and why?\n","\n","* In particular, would applying vertical or horizontal flips be appropriate for this dataset, given the robot’s camera viewpoint and driving environment?\n","\n","* Similarly, is scaling a suitable augmentation, or could it distort geometric relationships that are important for predicting steering angles?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2T3i38SmvJj9"},"outputs":[],"source":["# ============================================================\n","# Data Augmentations\n","# ============================================================\n","eval_tf  = make_eval_tf(224, 0.2)\n","train_tf = make_train_tf(argparse.Namespace(\n","    top_crop=...., #  0.1-0.3\n","    short_side=224,\n","    jitter_b= ...., # try values between 0.10-0.20\n","    jitter_c= ...., # try values between 0.10-0.20\n","    jitter_s= ...., # try values between 0.10-0.20\n","    jitter_h=0.02, # try values between 0.0 -0.5\n","    affine_deg=...., # try values between 0 - 10\n","    affine_trans=0.02, #\n","    affine_scale_min=0.95,\n","    affine_scale_max=1.05\n",")) if use_aug else eval_tf\n","\n","train_dl.dataset.img_tf = train_tf\n","if val_dl:\n","    val_dl.dataset.img_tf = eval_tf\n","if test_dl:\n","    test_dl.dataset.img_tf = eval_tf\n","\n","print(f\"Augmentations: {'ON' if use_aug else 'OFF'} | top_crop=20% | short_side=224\")"]},{"cell_type":"markdown","metadata":{"id":"DratwsDNvJj-"},"source":["The definition of the model is provided below. Fill in the details on the optimizer to be used, loss function and the scaler."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9zs09LfcvJj-"},"outputs":[],"source":["# ============================================================\n","# Model / Optimizer / Loss / AMP\n","# ============================================================\n","model = ImageOnlySteerNet(\n","    out_len=1,\n","    pretrained=pretrained,\n","    freeze_backbone=freeze_backbone,\n","    dropout=dropout,\n",").to(device)\n","\n","opt     = ##### initialize an optimizer for ur model. (Hint: AdamW is commonly used for Vision models)#####\n","loss_fn = ##### import a suitable regression loss function from pytorch #####\n","scaler  = ##### initialize gradscaler function#####\n","\n","writer = SummaryWriter(logdir)\n","writer.add_text(\"hparams\", str({\n","    \"lr\": lr, \"bs\": bs, \"epochs\": epochs,\n","    \"use_aug\": use_aug, \"dropout\": dropout\n","}))\n","writer.add_scalar(\"data/mu\", mu, 0)\n","writer.add_scalar(\"data/sigma\", sigma, 0)\n","\n","best_mae = float(\"inf\")\n","global_step = 0\n"]},{"cell_type":"markdown","metadata":{"id":"Vow-MsdblNYs"},"source":["### **TRAINING LOOP**\n","In this section, you will complete the model training loop.\n","The code below iterates through multiple epochs and performs:<br>\n","- Forward pass (model prediction)\n","- Loss computation\n","- Backward pass (gradient computation)\n","- Optimizer step (parameter update)\n","- Logging metrics to TensorBoard\n","\n","You are expected to:\n","- Implement the forward pass and loss calculation (TODOs below)\n","- Understand how mixed precision (autocast) and GradScaler work\n","- Track MAE and loss across batches\n","\n"," ============================================================\n","\n","### **VALIDATION, CHECKPOINT, AND TEST EVALUATION**\n","After each epoch of training, we evaluate model performance\n","on the validation (or training) set, log results to TensorBoard,\n","and save the best model checkpoint based on lowest MAE.\n","Finally, once training completes, we evaluate the final model\n","on the test dataset to estimate its generalization performance.\n","You must:<br>\n","• Understand how evaluate() is used to measure MAE/RMSE<br>\n","• Observe when and why checkpoints are saved\n","• Record test metrics and analyze how well the model learned"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wrh8oGYtkjLG"},"outputs":[],"source":["# ============================================================\n","# Training Loop\n","# ============================================================\n","for epoch in range(1, epochs + 1):\n","    model.train()\n","    running_loss = 0.0\n","    running_mae  = 0.0\n","    n_seen = 0\n","\n","    # Iterate over all batches\n","    for x, _, y_std, y_raw, _ in train_dl:\n","        x = x.to(device, non_blocking=True)\n","        y_std = y_std.to(device, non_blocking=True)\n","\n","        opt.zero_grad(set_to_none=True)\n","        with autocast(enabled=fp16):\n","                yhat_std = ...... # pass input images to perform forward pass\n","                loss = ........ # compute loss\n","\n","        ## Perform backward pass and update steps #####\n","\n","        # Log current batch loss and learning rate\n","        writer.add_scalar(\"train/step_loss\", loss.item(), global_step)\n","        lr_now = get_lr(opt)\n","        if lr_now is not None:\n","            writer.add_scalar(\"train/lr\", lr_now, global_step)\n","\n","        # Compute metrics (MAE in raw units)\n","        with torch.no_grad():\n","            yhat_raw = (yhat_std * sigma + mu).cpu()\n","            mae_batch = (yhat_raw - y_raw).abs().sum().item()\n","            running_mae  += mae_batch\n","            running_loss += loss.item() * y_std.shape[0]\n","            n_seen += y_std.shape[0]\n","\n","        global_step += 1\n","\n","    # ============================================================\n","    # Compute epoch averages\n","    # ============================================================\n","    train_loss = running_loss / max(1, n_seen)\n","    train_mae  = running_mae  / max(1, n_seen)\n","\n","    writer.add_scalar(\"train/epoch_loss\", train_loss, epoch)\n","    writer.add_scalar(\"train/epoch_MAE\",  train_mae,  epoch)\n","\n","    # ---- Validate ----\n","    if val_dl is not None:\n","            val_mae, val_rmse = evaluate(model, val_dl, device, mu, sigma)\n","    else:\n","            val_mae, val_rmse = evaluate(model, train_dl, device, mu, sigma)\n","\n","    writer.add_scalar(\"val/MAE\",  val_mae,  epoch)\n","    writer.add_scalar(\"val/RMSE\", val_rmse, epoch)\n","\n","    print(f\"[{epoch:03d}/{epochs}] \"\n","              f\"train_loss={train_loss:.4f}  train_MAE={train_mae:.4f}  \"\n","              f\"val_MAE={val_mae:.4f}  val_RMSE={val_rmse:.4f}\")\n","\n","    # ---- Checkpoint best by MAE ----\n","    if val_mae < best_mae:\n","            best_mae = val_mae\n","            torch.save({\n","                \"model_state\": model.state_dict(),\n","                \"mu\": mu,\n","                \"sigma\": sigma,\n","                \"omega_sign\": omega_sign,\n","                \"epoch\": epoch,\n","                \"val_mae\": val_mae,\n","                \"val_rmse\": val_rmse,\n","                \"args\": vars(args),\n","            }, ckpt_out)\n","            print(f\"  ↳ Saved best checkpoint to {ckpt_out} (MAE={val_mae:.4f})\")\n","            writer.add_scalar(\"ckpt/best_MAE\", best_mae, epoch)\n"]},{"cell_type":"markdown","metadata":{"id":"yktcJemJvJj-"},"source":["You have succesfully completed the training of the model. You can now vary the values of the following hyperparameters and study its effect on the model performance: <br>\n","- Epochs\n","- Learning Rate\n","- Weight Decay\n","- Dropout\n","\n","The values of the other hyper parameters need not be modified.\n","\n","Select the combination of hyperparameters that you think gives the best performance in terms of **Training Loss** and **Training Time**."]},{"cell_type":"markdown","metadata":{"id":"HSRKx6jcvJj-"},"source":["---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cDPTLMA8bkdJ"},"outputs":[],"source":["   # ---- Test ----\n","if test_dl is not None:\n","    test_mae, test_rmse = evaluate(model, test_dl, device, mu, sigma)\n","    print(f\"[TEST] MAE={test_mae:.4f}  RMSE={test_rmse:.4f}\")\n","    writer.add_scalar(\"test/MAE\",  test_mae,  args.epochs)\n","    writer.add_scalar(\"test/RMSE\", test_rmse, args.epochs)\n","else:\n","    print(\"No test split found; skipping test evaluation.\")\n","\n","writer.close()"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.18"}},"nbformat":4,"nbformat_minor":0}